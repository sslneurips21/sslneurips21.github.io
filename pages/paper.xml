<?xml version="1.0" encoding="utf-8"?>
<Workbook xmlns="urn:schemas-microsoft-com:office:spreadsheet" xmlns:x="urn:schemas-microsoft-com:office:excel" xmlns:ss="urn:schemas-microsoft-com:office:spreadsheet" xmlns:html="http://www.w3.org/TR/REC-html40">
  <Styles>
    <Style ss:ID="s1">
      <Font x:Family="Swiss" ss:Bold="1" />
    </Style>
    <Style ss:ID="s2">
      <NumberFormat ss:Format="0%" />
    </Style>
  </Styles>
  <Worksheet ss:Name="SSLNeurIPS2020">
    <Table>
      <Row>
        <Cell ss:Index="1">
          <Data ss:Type="String">SSLNeurIPS2020</Data>
        </Cell>
      </Row>
      <Row ss:Index="3">
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Paper ID</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Created</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Last Modified</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Paper Title</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Abstract</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Primary Contact Author Name</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Primary Contact Author Email</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Authors</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Author Names</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Author Emails</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Track Name</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Primary Subject Area</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Secondary Subject Areas</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Conflicts</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Assigned</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Completed</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">% Completed</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Bids</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Discussion</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Status</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Requested For Camera Ready</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Camera Ready Submitted?</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Requested For Author Feedback</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Author Feedback Submitted?</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Files</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Number of Files</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Supplementary Files</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Number of Supplementary Files</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Reviewers</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Reviewer Emails</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">MetaReviewers</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">MetaReviewer Emails</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">SeniorMetaReviewers</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">SeniorMetaReviewerEmails</Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[3]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[9/1/2020 4:42:43 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:52:45 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-Supervised Learning for Large-Scale Unsupervised Image Clustering]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts.  Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Evgenii Zheltonozhskii]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[evgeniizh@campus.technion.ac.il]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Evgenii Zheltonozhskii (Technion)*; Chaim Baskin (Technion); Alex Bronstein (Technion); Avi Mendelson (Technion)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Zheltonozhskii, Evgenii*; Baskin, Chaim; Bronstein, Alex; Mendelson, Avi]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[evgeniizh@campus.technion.ac.il*; chaimbaskin@cs.technion.ac.il; bron@cs.technion.ac.il; avi.mendelson@cs.technion.ac.il]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Unsupervised____NeurIPS_20_workshop(2).pdf (288,231 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jialong Tang (Institute of Software Chinese Academy of Sciences); Vitchyr Pong (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jialong2019@iscas.ac.cn; vitchyr@berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abdel-rahman Mohamed (Facebook AI Research (FAIR))]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abdo@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[4]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[9/28/2020 3:13:05 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/29/2020 2:16:27 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Learning Self-Expression Metrics for Scalable and Inductive Subspace Clustering]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Subspace clustering has established itself as a state-of-the-art approach to clustering high-dimensional data. In particular, methods relying on the self-expressiveness property have recently proved especially successful. However, they suffer from two major shortcomings: First, a quadratic-size coefficient matrix is learned directly, preventing these methods from scaling beyond small datasets. Secondly, the trained models are transductive and thus cannot be used to cluster out-of-sample data unseen during training. Instead of learning self-expression coefficients directly, we propose a novel metric learning approach to learn instead a subspace affinity function using a siamese neural network architecture. Consequently, our model benefits from a constant number of parameters and a constant-size memory footprint, allowing it to scale to considerably larger datasets. In addition, we can formally show that out model is still able to exactly recover subspace clusters given an independence assumption. The siamese architecture in combination with a novel geometric classifier further makes our model inductive, allowing it to cluster out-of-sample data. Additionally, non-linear clusters can be detected by simply adding an auto-encoder module to the architecture. The whole model can then be trained end-to-end in a self-supervised manner. This work in progress reports promising preliminary results on the MNIST dataset. In the spirit of reproducible research, me make all code publicly available. In future work we plan to investigate several extensions of our model and to expand experimental evaluation.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Julian Busch]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[busch@dbs.ifi.lmu.de]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Julian Busch (LMU Munich)*; Evgeniy Faerman (Ludwig Maximilian University of Munich); Matthias Schubert (Ludwig-Maximilians-Universität München); Thomas Seidl (LMU Munich)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Busch, Julian*; Faerman, Evgeniy; Schubert, Matthias; Seidl, Thomas]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[busch@dbs.ifi.lmu.de*; faerman@dbs.ifi.lmu.de; schubert@dbs.ifi.lmu.de; seidl@dbs.ifi.lmu.de]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Siamese_Subspace_Clustering_Networks___WIP_Workshop-2.pdf (405,311 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Xuehai He (UC San Diego); Yao-Hung Tsai (Carnegie Mellon University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[x5he@eng.ucsd.edu; yaohungt@cs.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[6]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[9/29/2020 10:39:12 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 3:24:33 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-alignment Pre-training for Biomedical Entity Representations]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Despite the widespread success of self-supervised learning via masked language models, learning representations directly from text to accurately capture complex and fine-grained semantic relationships in the biomedical domain remains as a challenge.  Addressing this is of paramount importance for tasks such as entity linking where complex relational knowledge is pivotal.  We propose SAPBERT, a pre-training scheme based on BERT.  It self-aligns the representation space of biomedical entities with a metric learning objective function leveraging UMLS, a collection of biomedical ontologies with >4M concepts. Our experimental results on six medical entity linking benchmarking datasets demonstrate that SAPBERT outperforms many domain-specific BERT-based variants such as BIOBERT, BLUEBERT and PUBMEDBERT, achieving the state-of-the-art (SOTA) performances.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Fangyu Liu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[fl399@cam.ac.uk]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Fangyu Liu (University of Cambridge)*; Ehsan Shareghi (UCL; University of Cambridge); Zaiqiao Meng (University of Glasgow); Marco Marco Basaldella (University of Cambridge); Nigel Collier (University of Cambridge)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Liu, Fangyu*; Shareghi, Ehsan; Meng, Zaiqiao; Marco Basaldella, Marco; Collier, Nigel]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[fl399@cam.ac.uk*; e.shareghi@ucl.ac.uk; zm324@cam.ac.uk; mb2313@cam.ac.uk; nhc30@cam.ac.uk]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[_NeurrIPS_SSL_workshop_2020__self_alignment_pretraining_v1.1.pdf (7,303,485 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Xin Wang (UC Berkeley); Xueting Li (University of California Merced)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[xinw@eecs.berkeley.edu; xuetingli1123@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[7]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[9/30/2020 5:42:42 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/29/2020 2:16:45 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Prototypical Contrastive Learning of Unsupervised Representations]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Junnan Li]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[junnan.li@salesforce.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Junnan Li (Salesforce)*; Pan Zhou (Salesforce); Caiming Xiong (Salesforce Research); Steven Hoi (Salesforce)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Li, Junnan*; Zhou, Pan; Xiong, Caiming; Hoi, Steven]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[junnan.li@salesforce.com*; pzhou@salesforce.com; cxiong@salesforce.com; shoi@salesforce.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[main.pdf (662,426 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ruohan Gao (University of Texas at Austin); Shikun Liu (Imperial College London)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[rhgao@cs.utexas.edu; sk.lorenmt@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ishan Misra (Facebook AI Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[imisra@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[8]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/2/2020 4:39:12 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:53:07 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[i-Mix: A Strategy for Regularizing Contrastive Representation Learning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We propose i-Mix, a simple yet effective regularization strategy for improving contrastive representation learning schemes in both vision and non-vision domains. In particular, we cast them as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, i-Mix provides more augmented data by mixing given inputs in the data space and their virtual class labels in the label space. Our experiments demonstrate that i-Mix consistently improves the quality of learned representations across different domains, which is often on par with end-to-end supervised learning on downstream tasks. Also, we investigate under what conditions i-Mix is effective.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kibok Lee]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[kibok@umich.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kibok Lee (University of Michigan)*; Yian Zhu (University of Michigan	); Kihyuk Sohn (Google); Chun-Liang Li (Google); Jinwoo Shin (KAIST); Honglak Lee (University of Michingan / Google Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Lee, Kibok*; Zhu, Yian; Sohn, Kihyuk; Li, Chun-Liang; Shin, Jinwoo; Lee, Honglak]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[kibok@umich.edu*; yianz@umich.edu; kihyuks@google.com; chunliang@google.com; jinwoos@kaist.ac.kr; honglak@eecs.umich.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[imix.pdf (655,877 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kyungjune Baek (Yonsei University); Vitchyr Pong (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[bkjbkj12@yonsei.ac.kr; vitchyr@berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abdel-rahman Mohamed (Facebook AI Research (FAIR))]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abdo@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[10]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/5/2020 3:06:33 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/29/2020 2:17:26 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[MixCo: Mix-up Contrastive Learning for Visual Representation]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Contrastive learning has shown remarkable results in recent self-supervised approaches for visual representation. By learning to contrast positive pairs' representation from the corresponding negatives pairs, one can train good visual representations without human annotations. This paper proposes Mix-up Contrast (MixCo), which extends the contrastive learning concept to semi-positives encoded from the mix-up of positive and negative images. MixCo aims to learn the relative similarity of representations, reflecting how much the mixed images have the original positives. We validate the efficacy of MixCo when applied to the recent self-supervised learning algorithms under the standard linear evaluation protocol on Tiny-ImageNet, CIFAR-100, and CIFAR-10. In the experiments, MixCo consistently improves test accuracy. Remarkably, the improvement is more significant when the learning capacity (e.g., model size) is limited, suggesting that MixCo might be more useful in real-world scenarios.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Gihun Lee]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[opcrisis@kaist.ac.kr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Gihun Lee (KAIST)*; Sungnyun Kim (KAIST); Sangmin Bae (KAIST); Se-Young Yun (KAIST)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Lee, Gihun*; Kim, Sungnyun; Bae, Sangmin; Yun, Se-Young]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[opcrisis@kaist.ac.kr*; ksn4397@kaist.ac.kr; bsmn0223@kaist.ac.kr; yunseyoung@kaist.ac.kr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[MixCo___Mix_up_Contrastive_Learning_for_Visual_Representation.pdf (612,700 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Vitchyr Pong (UC Berkeley); Xuehai He (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[vitchyr@berkeley.edu; x5he@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[12]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/6/2020 6:53:55 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 3:25:00 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-supervised Representation Learning with Relative Predictive Coding]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[For contrastive self-supervised learning objective functions, there exist three challenges that cannot be handled well simultaneously: training stability, sensitivity to minibatch size, and downstream task performance. Some objectives exhibit high variances that disrupt training, and others might have performance that is sensitive to minibatch size or lack strong empirical performance. This paper proposes Relative Predictive Coding (RPC), a new contrastive learning objective, that achieves a better balance among these three challenges. On vision and speech self-supervised learning benchmarks, we empirically show that RPC outperforms prior objectives and enjoys less minibatch size sensitivity and lower variance during training.
]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yao-Hung Tsai]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yaohungt@cs.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yao-Hung Tsai (Carnegie Mellon University)*; Martin Ma (Carnegie Mellon University); Muqiao Yang (Carnegie Mellon University); Han Zhao (Carnegie Mellon University); Ruslan Salakhutdinov (Carnegie Mellon University); Louis-Philippe Morency (Carnegie Mellon University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Tsai, Yao-Hung*; Ma, Martin; Yang, Muqiao; Zhao, Han; Salakhutdinov, Ruslan; Morency, Louis-Philippe]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yaohungt@cs.cmu.edu*; martin.q.ma1@pm.me; muqiaoy@cs.cmu.edu; han.zhao@cs.cmu.edu; rsalakhu@cs.cmu.edu; morency@cs.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[main.pdf (607,389 bytes); supple.pdf (371,201 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jiangliu WANG (CUHK); Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jiangliuwang@link.cuhk.edu.hk; p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[15]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/7/2020 9:44:43 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/29/2020 2:18:09 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Autoregressive language models pretrained on large corpora have been successful at solving downstream tasks, even with zero-shot usage. However, there is little theoretical justification for their success. This paper considers the following questions: (1) Why should learning the distribution of natural language help with downstream classification tasks? (2) Why do features learned using language modeling help solve downstream tasks with linear classifiers? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as next word prediction tasks, thus making language modeling a meaningful pretraining task. For (2), we analyze properties of the cross-entropy objective to show that $\epsilon$-optimal language models in cross-entropy (log-perplexity) learn features that are $\mathcal{O}(\sqrt{\epsilon})$-good on natural linear classification tasks, thus demonstrating mathematically that doing well on language modeling can be beneficial for downstream tasks. We perform experiments to verify assumptions and validate theoretical results. Our theoretical insights motivate a simple alternative to the cross-entropy objective that performs well on some linear classification tasks.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Nikunj Saunshi]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[nsaunshi@cs.princeton.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Nikunj Saunshi (Princeton University)*; Sadhika Malladi (Princeton); Sanjeev Arora (Princeton University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Saunshi, Nikunj*; Malladi, Sadhika; Arora, Sanjeev]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[nsaunshi@cs.princeton.edu*; smalladi@cs.princeton.edu; arora@cs.princeton.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[main.pdf (497,550 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Junhwa Hur (TU Darmstadt); Sina Mohseni (Texas A&M University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[junhwa.hur@visinf.tu-darmstadt.de; sina.mohseni@tamu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wenzhen Yuan ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuanwz@cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[16]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/8/2020 1:43:23 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 10:22:48 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[This paper introduces SelfMatch, a semi-supervised learning method that combines the power of contrastive self-supervised learning and consistency regularization. SelfMatch consists of two stages: (1) self-supervised pre-training based on contrastive learning and (2) semi-supervised fine-tuning based on augmentation consistency regularization. We empirically demonstrate that SelfMatch achieves the state-of-the-art results on standard benchmark datasets such as CIFAR-10 and SVHN. For example, for CIFAR-10 with 40 labeled examples, SelfMatch achieves 93.19% accuracy that outperforms the strong previous methods such as MixMatch (52.46%), UDA (70.95%), ReMixMatch (80.9%), and FixMatch (86.19%). We note that SelfMatch can close the gap between supervised learning (95.87%) and semi-supervised learning (93.19%) by using only a few labels for each class.  ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Byoungjip Kim]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[bjip.kim@samsung.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Byoungjip Kim (Samsung SDS)*; Jinho Choo (Samsung SDS); Yeong-Dae Kwon (Samsung SDS); Seongho Joe (samsung SDS); Seungjai Min (Samsung SDS); Youngjune L Gwon (Samsung SDS)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kim, Byoungjip*; Choo, Jinho; Kwon, Yeong-Dae; Joe, Seongho; Min, Seungjai; Gwon, Youngjune L]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[bjip.kim@samsung.com*; jinho12.choo@samsung.com; y.d.kwon@samsung.com; drizzle.cho@samsung.com; seungjai.min@samsung.com; gyj.gwon@samsung.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[NeurIPS_2020_SSL_SelfMatch_v9.pdf (255,111 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yuhua Chen (ETH Zurich); Yuning You (Texas A&M University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuhua.chen@vision.ee.ethz.ch; yuning.you@tamu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shanghang Zhang (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[shz@eecs.berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[17]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/8/2020 2:10:44 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:53:40 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Spatiotemporal Contrastive Video Representation Learning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We present a self-supervised Contrastive Video Representation Learning (CVRL) method to learn spatiotemporal visual representations from unlabeled videos. Our representations are learned using a contrastive loss, where two clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. We find both spatial and temporal augmentation are crucial for video self-supervised learning. In particular, we propose a simple yet effective temporally consistent spatial augmentation method to impose strong spatial augmentations on each frame of a video clip while maintaining the temporal consistency across frames.  CVRL shows superior performance on various tasks and datasets, e.g. for Kinetics-600 action recognition, a linear classifier trained on representations learned by CVRL achieves surprisingly 70.4% top-1 accuracy with a 3D-ResNet50 backbone, significantly outperforming ImageNet supervised pre-training and SimCLR unsupervised pre-training, and greatly closing the gap between unsupervised and supervised video representation learning.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Rui Qian]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[rq49@cornell.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Rui Qian (Cornell University)*; Tianjian Meng (Google Brain); Boqing Gong (Google); Ming-Hsuan Yang (Google Research); Huisheng Wang (Google); Serge Belongie (Cornell University); Yin Cui (Google)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Qian, Rui*; Meng, Tianjian; Gong, Boqing; Yang, Ming-Hsuan; Wang, Huisheng; Belongie, Serge; Cui, Yin]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[rq49@cornell.edu*; mengtianjian@google.com; bgong@google.com; minghsuan@google.com; huishengw@google.com; sjb344@cornell.edu; yincui@google.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[CVRL.pdf (292,773 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Fei Pan (Korea Advanced Institute of Science and Technology); Zhongzheng Ren (UIUC)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[feipan@kaist.ac.kr; zr5@illinois.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abdel-rahman Mohamed (Facebook AI Research (FAIR))]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abdo@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[18]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/8/2020 3:14:52 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/29/2020 2:18:26 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Make Lead Bias in Your Favor: Zero-shot Abstractive News Summarization]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pre-train abstractive news summarization models on large-scale unlabeled corpus: predicting the leading sentences using the rest of an article. We collect a massive news corpus and conduct careful data cleaning and filtering. We then apply the proposed self-supervised pre-training to existing generation models BART and T5. Via extensive experiments on six benchmark datasets, we show that this approach can dramatically improve the quality of summary and achieve state-of-the-art results for zero-shot news summarization without any fine-tuning. For example, in the DUC-2003 dataset, the ROUGE-1 of BART increases 13.7% after the lead-bias pre-training.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Chenguang Zhu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[chezhu@microsoft.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Chenguang Zhu (Microsoft)*; Ziyi Yang (Stanford University); Robert Gmyr (Microsoft); Michael Zeng (Microsoft); Xuedong Huang (Microsoft)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Zhu, Chenguang*; Yang, Ziyi; Gmyr, Robert; Zeng, Michael; Huang, Xuedong]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[chezhu@microsoft.com*; zy99@stanford.edu; rogmyr@microsoft.com; nzeng@microsoft.com; xdh@microsoft.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Leadbias_NIPS.pdf (152,616 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Biagio Brattoli (Heidelberg University); Yao-Hung Tsai (Carnegie Mellon University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[biagio.brattoli@gmail.com; yaohungt@cs.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wenzhen Yuan ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuanwz@cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[20]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 1:49:58 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 9:50:44 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Semantic Augmentation with Self-Supervised Content Mixing for Semi-Supervised Learning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Leveraging unlabeled samples is a crucial issue for improving performances in semi-supervised learning. We introduce the SAMOSA framework that adds novel self-supervised reconstruction modules and loss terms to facilitate semantic augmentation mixing semantic components from labeled samples and non semantic characteristics from unlabeled ones. We demonstrate the effectiveness of SAMOSA on standard SSL procedures Mean Teacher and MixMatch.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Remy Sun]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[remy.sun@ens-rennes.fr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Remy Sun (Laboratoire d'informatique de Paris 6)*; Clément Masson (Thales Land and Air Systems); Gilles Henaff (Thales Optronique S.A.S.); Nicolas Thome (Cnam, CEDRIC); Matthieu Cord (Sorbonne University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Sun, Remy*; Masson, Clément; Henaff, Gilles; Thome, Nicolas; Cord, Matthieu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[remy.sun@ens-rennes.fr*; clement.masson@fr.thalesgroup.com; gilles.henaff@fr.thalesgroup.com; nicolas.thome@lecnam.net; matthieu.cord@lip6.fr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[sslneurips.pdf (1,391,529 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shikun Liu (Imperial College London); Xueting Li (University of California Merced)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[sk.lorenmt@gmail.com; xuetingli1123@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shanghang Zhang (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[shz@eecs.berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[21]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 4:42:20 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:54:00 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-Supervised Object-Wise 3D Decomposition of Images using Shape Priors]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Object-wise scene representations are essential for scene understanding and decision making by intelligent agents. In this paper, we present our approach for learning multi-object scene representations from images which model the 3D scene layout and the shape and texture of objects. Our deep recurrent architecture encodes single images into an object-wise latent representation with 3D shapes, poses and texture of the objects. We confine the space of possible shapes using a pre-trained shape prior represented continuously in function-space as signed distance functions.  The scene representation is rendered back into images in a differentiable way to facilitate self-supervised learning of scene decomposition. We evaluate our approach on scenes generated with ShapeNet models.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Cathrin Elich]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[cathrin.elich@tuebingen.mpg.de]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Cathrin Elich (Max Planck Institute for Intelligent Systems)*; Martin R. Oswald (ETH Zurich); Marc Pollefeys (ETH Zurich / Microsoft); Joerg Stueckler (Max-Planck-Institute for Intelligent Systems)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Elich, Cathrin*; Oswald, Martin R.; Pollefeys, Marc; Stueckler, Joerg]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[cathrin.elich@tuebingen.mpg.de*; martin.oswald@inf.ethz.ch; marc.pollefeys@inf.ethz.ch; joerg.stueckler@tuebingen.mpg.de]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[neurips_ssl_2020submission.pdf (611,572 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Suraj Nair (Stanford University); Yue Wang (Massachusetts Institute of Technology)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[surajn@stanford.edu; yuewang@csail.mit.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abdel-rahman Mohamed (Facebook AI Research (FAIR))]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abdo@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[24]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 7:45:21 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 9:50:13 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Contrastive Learning with Hard Negative Samples]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We consider the question: how can you sample good negative examples for contrastive learning? We argue that, as with metric learning, learning contrastive representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use label information. In response, we develop a new class of unsupervised methods for selecting hard negative  samples where the user can control the amount of hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves the generalization of visual representations, requires only few additional lines of code to implement, and introduces no computational overhead. ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Joshua D Robinson]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[joshrob@mit.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Joshua D Robinson (MIT)*; Ching-Yao  Chuang (MIT); Suvrit Sra (Massachusetts Institute of Technology, USA); Stefanie  Jegelka (MIT)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Robinson, Joshua D*; Chuang, Ching-Yao ; Sra, Suvrit; Jegelka, Stefanie ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[joshrob@mit.edu*; cychuang@mit.edu; suvrit@mit.edu; stefje@csail.mit.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[main.pdf (353,568 bytes); supplementary.pdf (368,959 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego); Xingyi Yang (University of California San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu; x3yang@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shanghang Zhang (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[shz@eecs.berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[25]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 10:58:16 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 3:25:49 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[BYOL works even without batch statistics]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Bootstrap Your Own Latent (BYOL) is a self-supervised learning approach for image representation. From an augmented view of an image, BYOL trains an online network to predict a target network representation of a different augmented view of the same image. Unlike contrastive methods, BYOL does not explicitly use a repulsion term built from negative pairs in its training objective, and yet avoids collapse to a trivial, constant representation. Thus, it has recently been hypothesized that Batch Normalization (BN) is critical to prevent collapse in BYOL. Indeed, BN flows gradients across  batch elements, and could leak information about negative views in the batch, which could act as an implicit negative (contrastive) term. However, we experimentally show that replacing BN with a batch-independent normalization scheme (namely, a combination of group normalization and weight standardization achieves performance comparable to vanilla BYOL (73.1% vs. 74.3% top-1 accuracy under the linear evaluation protocol on ImageNet with ResNet-50). This  finding disproves the hypothesis that the use of batch statistics is a crucial ingredient for BYOL to learn useful representations.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pierre H. Richemond]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[pierre.richemond@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pierre H. Richemond (DeepMind)*; Jean-Bastien Grill (DeepMind); Florent Altché (DeepMind); Corentin Tallec (Deepmind); Florian Strub (DeepMind); Bilal Piot (DeepMind); Michal Valko (DeepMind)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Richemond, Pierre H.*; Grill, Jean-Bastien; Altché, Florent; Tallec, Corentin; Strub, Florian; Piot, Bilal; Valko, Michal]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[pierre.richemond@gmail.com*; jbgrill@google.com; altche@google.com; corentint@google.com; florian.strub@gmail.com; piot@google.com; valkom@google.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[BYOL_works_without_batch_statistics___NeurIPS_2020_Workshop__Self_Supervised_Learning___Theory_and_Practice.pdf (255,173 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego); Yuki Asano (University of Oxford)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu; yuki@robots.ox.ac.uk]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[28]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 1:29:36 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 9:51:04 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-supervised Learning from a Multi-view Perspective]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[As a subset of unsupervised representation learning, self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as object detection and image captioning. Many proposed approaches for self-supervised learning follow naturally a multi-view perspective, where the input (e.g., original images) and the self-supervised signals (e.g., augmented images) can be seen as two redundant views of the data. Building from this multi-view perspective, this paper provides an information-theoretical framework to better understand the properties that encourage successful self-supervised learning. Specifically, we demonstrate that self-supervised learned representations can extract task-relevant information and discard task-irrelevant information. Our theoretical framework paves the way to a larger space of self-supervised learning objective design. In particular, we propose a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduce an additional objective term to discard task-irrelevant information. To verify our analysis, we conduct controlled experiments to evaluate the impact of the composite objectives. ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yao-Hung Tsai]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yaohungt@cs.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yao-Hung Tsai (Carnegie Mellon University)*; Yue Wu (Carnegie Mellon University); Ruslan Salakhutdinov (Carnegie Mellon University); Louis-Philippe Morency (Carnegie Mellon University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Tsai, Yao-Hung*; Wu, Yue; Salakhutdinov, Ruslan; Morency, Louis-Philippe]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yaohungt@cs.cmu.edu*; ywu5@andrew.cmu.edu; rsalakhu@cs.cmu.edu; morency@cs.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[NeurIPS_Workshop_Understanding_SSL_main.pdf (785,821 bytes); NeurIPS_Workshop_Understanding_SSL_supple.pdf (540,185 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shaolei Wang (Harbin Institute of Technology); Yuhua Chen (ETH Zurich)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[slwang@ir.hit.edu.cn; yuhua.chen@vision.ee.ethz.ch]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shanghang Zhang (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[shz@eecs.berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[29]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 1:44:46 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:54:18 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Refining Pre-trained NLP Models Through Shuffled-token Detection]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[State-of-the-art transformer models have achieved robust performance on a variety of NLP tasks. Many of these approaches have employed domain agnostic pre-training tasks to train models that yield highly generalized sentence representations that can be fine-tuned for specific downstream tasks. We propose refining a pre-trained NLP model using the objective of detecting shuffled tokens. We use a sequential approach by starting with the pre-trained RoBERTa model and training it using our approach. Applying random shuffling strategy on the word-level, we found that our approach enables the RoBERTa model achieve better performance than our baseline on 5 out of 7 GLUE tasks. Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Subhadarshi Panda]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[spanda@gradcenter.cuny.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Subhadarshi Panda (Graduate Center City University of New York)*; Anjali Agrawal (New York University); Jeewon Ha (New York University); Benjamin Bloch (New York University); Samuel R. Bowman (New York University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Panda, Subhadarshi*; Agrawal, Anjali; Ha, Jeewon; Bloch, Benjamin; Bowman, Samuel R.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[spanda@gradcenter.cuny.edu*; aa7513@nyu.edu; jh6926@nyu.edu; bb1976@nyu.edu; bowman@nyu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Neurips 2020 workshop draft B.pdf (272,515 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Junhwa Hur (TU Darmstadt); Yuning You (Texas A&M University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[junhwa.hur@visinf.tu-darmstadt.de; yuning.you@tamu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abdel-rahman Mohamed (Facebook AI Research (FAIR))]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abdo@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[32]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 2:30:15 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 3:26:39 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Understanding Self-supervised Learning with Dual Deep Networks]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations, which we show leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same  framework, we also show analytically that BYOL works due to an implicit contrastive term, acting as an approximate covariance operator. The term is formed by the inter-play between the zero-mean operation of BatchNorm and the extra predictor in the online network. Extensive ablation studies justify our theoretical findings.  ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yuandong Tian]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuandong@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yuandong Tian (Facebook)*; Lantao Yu (Stanford University); Xinlei Chen (Facebook AI Research); Surya Ganguli ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Tian, Yuandong*; Yu, Lantao; Chen, Xinlei; Ganguli, Surya]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuandong@fb.com*; lantaoyu@cs.stanford.edu; xinleic@fb.com; sganguli@stanford.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ssl_workshop.pdf (942,818 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Biagio Brattoli (Heidelberg University); Ruohan Gao (University of Texas at Austin)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[biagio.brattoli@gmail.com; rhgao@cs.utexas.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ishan Misra (Facebook AI Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[imisra@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[33]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 2:57:01 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 3:27:02 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Visual Question Answering with Annotation-Efficient Zero Shot Learning under Linguistic Domain Shift]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Heavy reliance on human-annotated training datasets (which typically suffer from annotator subjectivity and linguistic priors) has led to learning spurious correlations, bias amplification, and lack of robustness in vision-and-language (V\&L) models.
We study whether VQA models can be trained without any human-annotated Q-A pairs or object-bounding boxes.  We use a self-supervised framework that involves procedural synthesis of Q-A pairs from captions and pre-training tasks for training our models. Since our Q-A pairs are synthetic, they exhibit a linguistic domain shift from the questions in VQA data and a label-shift in the answer-set, i.e. a zero-shot learning task. We benchmark our models on VQA-v2, GQA, and on VQA-CP which contains a softer version of label shift. ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pratyay Banerjee]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[pbanerj6@asu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pratyay Banerjee (Arizona State University)*; Tejas Gokhale (Arizona State University); Yezhou Yang (Arizona State University); Chitta Baral (Arizona State University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Banerjee, Pratyay*; Gokhale, Tejas; Yang, Yezhou; Baral, Chitta]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[pbanerj6@asu.edu*; tgokhale@asu.edu; yz.yang@asu.edu; chitta@asu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[neurips_workshop_ssl.pdf (980,891 bytes); supplementary_appendix.pdf (2,994,928 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shikun Liu (Imperial College London); Xin Wang (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[sk.lorenmt@gmail.com; xinw@eecs.berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[34]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 4:37:23 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:54:52 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Data Transformation Insights in Self-Supervision with Clustering Tasks]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-supervision is key to extending use of deep learning for label scarce domains. For most of self-supervised approaches, data transformations play an important role. However, up until now the impact of transformations have not been studied theoretically and very sparsely empirically. We show theoretically and empirically that certain set of transformations are helpful in convergence of self-supervised clustering. We also show the cases when the transformations are not helpful or in some cases even harmful. We show faster convergence rate with valid transformations for convex as well as certain family of non-convex objectives along with the proof of convergence. We have synthetic as well as real world data experiments. Empirically our results conform with our theoretical insights.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abhimanu Kumar]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abhimank@cs.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abhimanu Kumar (Microsoft)*; Aniket Anand Deshmukh (Microsoft); Urun Dogan (Microsoft); Denis Charles (Microsoft); Eren Manavoglu ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kumar, Abhimanu*; Deshmukh, Aniket Anand; Dogan, Urun; Charles, Denis; Manavoglu, Eren]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abhimank@cs.cmu.edu*; aniketde@umich.edu; urun.dogan@skype.net; dxcharles@gmail.com; erenmanavoglu@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSTheoryClustering_NeurIPSSS2020.pdf (465,432 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Haoyu Ma (University of California, Irvine); Xingyi Yang (University of California San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[haoyum3@uci.edu; x3yang@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[36]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 5:21:59 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:55:31 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Functional Regularization for Representation Learning: A Unified Theoretical Perspective]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Unsupervised and self-supervised learning approaches have become a crucial tool to learn representations for downstream prediction tasks. We present a unifying theoretical perspective where several such approaches can be viewed as imposing a regularization on the representation via a learnable function using unlabeled data. We propose a discriminative framework, inspired from (Balcan and Blum 2010), for analyzing the sample complexity of these approaches. Our sample complexity bounds show that, with carefully chosen hypothesis classes to exploit the structure in the data, such functional regularization can prune the hypothesis space and help reduce the labeled data needed. We provide two concrete examples of functional regularization, using auto-encoders and masked self-supervision, and apply the framework to quantify the reduction in the sample complexity bound. We also provide complementary empirical results to support our analysis on synthetic and real data.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Siddhant Garg]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[sidgarg@amazon.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Siddhant Garg (Amazon Alexa AI Search)*; Yingyu Liang (University of Wisconsin Madison)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Garg, Siddhant*; Liang, Yingyu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[sidgarg@amazon.com*; yliang@cs.wisc.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSL Workshop Func Reg.pdf (1,904,887 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego); Suraj Nair (Stanford University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu; surajn@stanford.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abdel-rahman Mohamed (Facebook AI Research (FAIR))]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abdo@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[37]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/9/2020 9:35:16 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:55:40 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Cross-Domain Sentiment Classification With In-domain Contrastive Learning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Cross-domain sentiment classification has gained considerable attention recently. In this paper, we propose a contrastive learning framework for cross-domain sentiment classification. We aim to induce domain invariant optimal classifiers rather than distribution matching. To this end, we introduce in-domain contrastive learning and entropy minimization. Also, we find through ablation studies that these two techniques behaviour differently in case of large label distribution shift and conclude that the best practice is to choose one of them adaptively according to label distribution shift. The new state-of-the-art results our model achieves on standard benchmarks show the efficacy of the proposed method. ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Tian Li]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[davidli@pku.edu.cn]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Tian Li (Peking University)*; Xiang Chen (Peking University); Zhen Dong (UC Berkeley); Shanghang Zhang (UC Berkeley); Kurt Keutzer (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Li, Tian*; Chen, Xiang; Dong, Zhen; Zhang, Shanghang; Keutzer, Kurt]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[davidli@pku.edu.cn*; caspar@pku.edu.cn; zhendong@berkeley.edu; shz@eecs.berkeley.edu; keutzer@eecs.berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Neurips_SSL_workshop_submission v2.3.pdf (621,855 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[rebuttal of cross-domain sentiment classification with in-domain contrastive learning.pdf (35,988 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Santhosh Kumar Ramakrishnan (University of Texas at Austin); Victoria Dean (Carnegie Mellon University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[srama@cs.utexas.edu; vdean@cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[38]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 1:58:36 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:55:51 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[At the heart of many robotics problems is the challenge of learning correspondences across domains. For instance, imitation learning requires obtaining correspondence between humans and robots, sim-to-real requires correspondence between physics simulators and real hardware, transfer learning requires correspondences between different robot environments. In this work, we propose to learn correspondence across such domains with an emphasis on differing modalities (vision and internal state), physics parameters (mass and friction), and morphologies (number of limbs). Importantly, correspondences are learned using unpaired and randomly collected data from the two domains. To do so, we propose dynamics cycles that align dynamic robotic behavior across two domains using a cycle consistency constraint. Once this correspondence is found, we can directly transfer the policy trained on one domain to the other, without needing any additional fine-tuning on the second domain. We perform experiments across a variety of problem domains, both in simulation and on real robots. Particularly, our framework is able to align uncalibrated monocular video of a real robot arm to dynamic state-action trajectories of a simulated arm without paired data.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Qiang Zhang]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[zhangqiang2016@sjtu.edu.cn]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Qiang Zhang (Shanghai Jiao Tong University)*; Tete Xiao (UC Berkeley); Alexei A Efros (UC Berkeley); Lerrel Pinto (NYU/Berkeley); Xiaolong Wang (UCSD)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Zhang, Qiang*; Xiao, Tete; Efros, Alexei A; Pinto, Lerrel; Wang, Xiaolong]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[zhangqiang2016@sjtu.edu.cn*; txiao@eecs.berkeley.edu; efros@eecs.berkeley.edu; lerrel@cs.nyu.edu; xiw012@ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[cycle_dynamics.pdf (1,106,410 bytes); cycle_dynamics_appendix.pdf (483,744 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Hankook Lee (KAIST); Ke Sun (Peking University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[hankook.lee@kaist.ac.kr; ajksunke@pku.edu.cn]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ishan Misra (Facebook AI Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[imisra@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[39]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 2:32:55 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:56:05 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pretraining Neural Architecture Search Controllers with Locality-based Self-Supervised Learning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Neural architecture search (NAS) has fostered various fields of machine learning. Despite its prominent dedications, many have criticized the intrinsic limitations of high computational cost. We aim to ameliorate this by proposing a pretraining scheme that can be generally applied to controller-based NAS. Our method, locality-based self-supervised classification task, leverages the structural similarity of network architectures to obtain good architecture representations. We incorporate our method to neural architecture optimization (NAO) to analyze the pretrained embeddings and its effectiveness and highlight that adding metric learning loss brings a favorable impact on NAS. Our code is available at \url{https://}.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kwanghee Choi]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[juice500@sogang.ac.kr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kwanghee Choi (Sogang University)*; Minyoung Choe (Sogang university); Hyelee Lee (NAVER)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Choi, Kwanghee*; Choe, Minyoung; Lee, Hyelee]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[juice500@sogang.ac.kr*; choecindy05@gmail.com; esara2021@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[NIPSW_SSL_2020.pdf (477,311 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego); Tianlong Chen (Unversity of Texas at Austin)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu; tianlong.chen@utexas.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[41]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 4:28:25 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 8:22:39 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[In this paper, we teach machines to understand visuals and natural language by learning the mapping between sentences and noisy video snippets without explicit annotations. Firstly, we define a self-supervised learning framework that captures the cross-modal information. A novel adversarial learning module is then introduced to explicitly handle the noises in the natural videos, where the subtitle sentences are not guaranteed to be strongly corresponded to the video snippets. For training and evaluation, we contribute a new dataset `ApartmenTour' that contains a large number of online videos and subtitles. We carry out experiments on the bidirectional retrieval tasks between sentences and videos, and the results demonstrate that our proposed model achieves the state-of-the-art performance on both retrieval tasks and exceeds several strong baselines.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yujie Zhong]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yujie@robots.ox.ac.uk]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yujie Zhong (University of Oxford)*; Yishu Miao (Imperial College London); Linhai Xie (University of Oxford); Sen Wang (Heriot-Watt University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Zhong, Yujie*; Miao, Yishu; Xie, Linhai; Wang, Sen]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yujie@robots.ox.ac.uk*; yshu.miao@gmail.com; xielinhai@gmail.com; s.wang@hw.ac.uk]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[WAL_nips2020.pdf (1,460,863 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Haoyu Ma (University of California, Irvine); Sina Mohseni (Texas A&M University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[haoyum3@uci.edu; sina.mohseni@tamu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shanghang Zhang (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[shz@cs.berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[42]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 4:31:54 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:56:31 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Representation Learning via Invariant Causal Mechanisms]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[In this paper we analyze self-supervised representation learning using a causal framework.
We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. 
Based on this, we propose a novel self-supervised objective, 
Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer
which yields improved generalization guarantees.
Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of contrastive methods.
Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on 51 out of 57 games. ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jovana Mitrovic]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[mitrovic@google.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jovana Mitrovic (DeepMind)*; Brian McWilliams (DeepMind); Jacob Walker (DeepMind); Lars Buesing (DeepMind); Charles  Blundell (DeepMind)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Mitrovic, Jovana*; McWilliams, Brian; Walker, Jacob; Buesing, Lars; Blundell , Charles ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[mitrovic@google.com*; bmcw@google.com; jcwalker@google.com; lbuesing@google.com; cblundell@google.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[relic_ssl_workshop.pdf (3,390,293 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shaolei Wang (Harbin Institute of Technology); Yuki Asano (University of Oxford)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[slwang@ir.hit.edu.cn; yuki@robots.ox.ac.uk]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abdel-rahman Mohamed (Facebook AI Research (FAIR))]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abdo@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[43]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 7:22:05 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 10:27:35 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSD: A Unified Framework for Self-Supervised Outlier Detection]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We ask the following question: what training information is required to design an effective outlier / out-of-distribution (OOD) detector, i.e, detecting samples that lie far away from training distribution? Since unlabeled data is easily accessible for many applications, the most compelling approach is to develop detectors based on only unlabeled in-distribution data. However, we observe that existing detectors based on unlabeled data perform poorly, often equivalent to a random prediction. In contrast, existing state-of-the-art OOD detectors achieve impressive performance but require access to fine-grained data labels for supervised training. We propose SSD, an outlier detector based on only unlabeled training data. We use self-supervised representation learning followed by a Mahalanobis distance based detection in the feature space. We demonstrate that SSD outperforms existing detectors based on unlabeled data by a large margin. Additionally, SSD achieves performance on par, and sometimes even better, with supervised training based detectors.  Finally, we expand our detection framework with two key extensions. First, we formulate few-shot OOD detection, in which the detector has access to only one to five samples from the targeted OOD dataset. Second, we extend our framework to incorporate training data labels, if available. We find that our novel detection framework based on SSD displays enhanced performance with these extensions, and achieves state-of-the-art performance.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Vikash Sehwag]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[vvikash@princeton.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Vikash Sehwag (Princeton University)*; Mung Chiang (Princeton University); Prateek Mittal (Princeton University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Sehwag, Vikash*; Chiang, Mung; Mittal, Prateek]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[vvikash@princeton.edu*; chiangm@princeton.edu; pmittal@princeton.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSD__self_supervised_outlier_detection__NeurIPS_SSL_workshop_.pdf (676,473 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yuhua Chen (ETH Zurich); Yuning You (Texas A&M University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuhua.chen@vision.ee.ethz.ch; yuning.you@tamu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shanghang Zhang (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[shz@cs.berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[45]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 8:33:42 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:56:58 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Is It a Plausible Colour? UCapsNet for Colourisation]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Human beings can imagine the colours of a grayscale image with no particular effort thanks to their ability of semantic feature extraction. Can an autonomous system achieve that? Can it hallucinate plausible and vibrant colours?
This is the colourisation problem.
Different from existing works relying on convolutional neural networks models pre-trained with supervision, we cast such colourisation problem as a self-supervised learning task.
We tackle the problem with the introduction of a novel architecture based on Capsules trained following the adversarial learning paradigm.
Capsule networks are able to extract a semantic representation of the entities in the image but loose details about their spatial information, which is important for colourising a grayscale image.
Thus our UCapsNet structure comes with an encoding phase that extracts entities through capsules and spatial details through convolutional neural networks.
A decoding phase merges the entities features with the spatial features to hallucinate a plausible colour version of the input datum.
Results on the ImageNet benchmark show that our approach is able to generate more vibrant colours and plausible colours than exiting solutions and achieves superior performance than models pre-trained with supervision.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Rita Pucci]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[rita.pucci@uniud.it]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Rita Pucci (University of Udine)*; CHRISTIAN MICHELONI (University of Udine, Italy); Gian Luca Foresti (University of Udine, Italy); Niki Martinel (University of Udine)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pucci, Rita*; MICHELONI, CHRISTIAN; Foresti, Gian Luca; Martinel, Niki]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[rita.pucci@uniud.it*; christian.micheloni@uniud.it; gianluca.foresti@uniud.it; niki.martinel@uniud.it]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS_2020_RP.pdf (759,488 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Peter West (University of Washington); Zhiqiu Lin ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[pawest@cs.washington.edu; zhiqiul@andrew.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wenzhen Yuan ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuanwz@cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[46]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 10:16:26 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:57:12 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-guided Approximate Linear Programs]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Approximate linear programs (ALPs) are well-known models based on value function approximations (VFAs) to obtain heuristic policies and lower bounds on the optimal policy cost of Markov decision processes (MDPs). The ALP VFA is a linear combination of predefined basis functions that are chosen using domain knowledge and updated heuristically if the ALP optimality gap is large. We side-step the need for such basis function engineering in ALP - an implementation bottleneck - by proposing a sequence of ALPs that embed increasing numbers of random basis functions obtained via inexpensive sampling. We provide a sampling guarantee and show that the VFAs from this sequence of models converge to the exact value function. We also show under mild conditions that our ALP policy cost is near-optimal when the number of sampled random bases is sufficiently large. Nevertheless, the performance of this ALP policy can fluctuate significantly as more basis functions are sampled. To mitigate these fluctuations, we "self-guide" our convergent sequence of ALPs using past VFA information such that a worst-case measure of policy performance is improved. Moreover, our method provides application-agnostic policies and bounds to benchmark approaches that exploit application structure.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Parshan Pakiman]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ppakim2@uic.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Parshan Pakiman (University of Illinois at Chicago)*; Selvaprabu Nadarajah (	University of Illinois at Chicago); Negar Soheili Azad (University of Illinois at Chicago); Qihang Lin (University of Iowa)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pakiman, Parshan*; Nadarajah, Selvaprabu; Soheili Azad, Negar; Lin, Qihang]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ppakim2@uic.edu*; selvan@uic.edu; nazad@uic.edu; qihang-lin@uiowa.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[self_guided_ALPs_NeurIPS_ Self_supervised_Learning.pdf (582,846 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jin Sun (Cornell University); Zhiqiu Lin ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jinsun@cornell.edu; zhiqiul@andrew.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wenzhen Yuan ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuanwz@cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[48]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 10:45:23 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:57:46 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SemanticCMC - improved semantic self-supervised learning with naturalistic temporal co-occurrences]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-supervised learning is advancing state-of-the-art in machine learning methods, lessening our reliance on curated datasets. To understand why and how these models are performing well it is useful to examine the representational or cognitive foundations on which they execute their behaviours. Here, we present an application of Representational Similarity Analysis (RSA) to investigate the patterns of activations within a top-performing self-supervised network, Contrastive Multiview Coding (CMC). We illustrate that, despite enabling high ImageNet classification accuracy, purely perceptual auxiliary tasks prevent a self-supervised network such as CMC from capturing more high-level semantic structure. We present SemanticCMC, trained on a naturalistic movie dataset with meaningful temporal structure. We illustrate that this semantic task improves coding of concept semantics despite its attenuated classification accuracy. This preliminary analysis on a single self-supervised network highlights that reliance on object-level decoding does not always indicate that meaningful structure has been captured. By investigating these cognitive underpinnings of how artificial networks represent concepts, we can improve theoretical understanding of SSL and motivate its engineering progress for a breadth of applications.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Cliona O'Doherty]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[odoherc1@tcd.ie]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Cliona O'Doherty (Trinity College Dublin)*; Rhodri Cusack (Trinity College Dublin)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[O'Doherty, Cliona*; Cusack, Rhodri]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[odoherc1@tcd.ie*; cusackrh@tcd.ie]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[48_SSL_NeurIPS.pdf (166,653 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jin Sun (Cornell University); Peter West (University of Washington)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jinsun@cornell.edu; pawest@cs.washington.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ishan Misra (Facebook AI Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[imisra@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[49]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 12:39:06 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:58:00 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Social NCE: Contrastive Learning for Socially-aware Trajectory Forecasting and Motion Planning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Understanding and anticipating the motion of dynamic agents like pedestrians and vehicles is crucial for autonomous robots to navigate safely and naturally in crowded spaces. Recent works have shown the promise of designing sequential predictive models with neural networks for socially-aware trajectory forecasting and motion planning. Yet, existing models are often brittle and lack common sense about collisions. In this work, we propose a simple but effective technique to improve the extracted social representation via contrastive learning. Unlike previous methods that only learn from preferred motions exhibited in the training set, we introduce an additional contrastive loss, dubbed Social-NCE, that encourages the encoded representation to be maximally useful for distinguishing a positive scenario of the future from a set of synthetic negatives that are socially prohibitive or undesirable. This approach allows for directly incorporating our prior knowledge into social representation learning through the design of negative samples. Experimental results show that the proposed Social-NCE can consistently improve the robustness and accuracy of previous trajectory forecasting and behavioral cloning methods. When applied to reinforcement learning, our method also boosts the performance of prior work on crowd navigation.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yuejiang Liu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuejiang.liu@epfl.ch]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yuejiang Liu (EPFL)*; Qi Yan (EPFL); Alexandre Alahi (EPFL)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Liu, Yuejiang*; Yan, Qi; Alahi, Alexandre]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuejiang.liu@epfl.ch*; qi.yan@epfl.ch; alexandre.alahi@epfl.ch]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SocialNCE.pdf (519,502 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Fei Pan (Korea Advanced Institute of Science and Technology); Yao-Hung Tsai (Carnegie Mellon University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[feipan@kaist.ac.kr; yaohungt@cs.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ishan Misra (Facebook AI Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[imisra@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[50]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 12:50:35 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:58:13 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-Supervised Ranking for Representation Learning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We present a new framework for self-supervised representation learning by positing it as a ranking problem in an image retrieval context on a large number of random views from random sets of images. Our work is based on two intuitive observations: first, a good representation of images must yield a high-quality image ranking in a retrieval task; second, we would expect random views of an image to be ranked closer to a reference view of that image than random views of other images. Hence, we model representation learning as a learning-to-rank problem in an image retrieval context, and train it by maximizing average precision (AP) for ranking. Specifically, given a mini-batch of images, we generate a large number of positive/negative samples and calculate a ranking loss term by separately treating each image view as a retrieval query. The new framework, dubbed S2R2, enables computing a global objective compared to the local objective in the popular contrastive learning framework calculated on pairs of views. A global objective leads S2R2 to faster convergence in terms of the number of epochs. In principle, by using a ranking criterion, we eliminate reliance on object-centered curated datasets (e.g., ImageNet). When trained on STL10 and MS-COCO, S2R2 outperforms SimCLR and performs on par with the state-of-the-art clustering-based contrastive learning model, SwAV, while being much simpler both conceptually and implementation-wise. Furthermore, when trained on a small subset of MS-COCO with fewer similar scenes, S2R2 significantly outperforms both SwAV and SimCLR. This indicates that S2R2 is potentially more effective on diverse scenes and decreases the need for a large training dataset for self-supervised learning.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ali Varamesh]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[alivaramesh@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ali Varamesh (KU Leuven)*; Ali Diba (KU Leuven); Tinne Tuytelaars (KU Leuven); Luc Van Gool (ETH Zurich)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Varamesh, Ali*; Diba, Ali; Tuytelaars, Tinne; Van Gool, Luc]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[alivaramesh@gmail.com*; ali.diba@kuleuven.be; Tinne.Tuytelaars@esat.kuleuven.be; vangool@vision.ee.ethz.ch]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[NeurIPS2020SSL.pdf (481,086 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Biagio Brattoli (Heidelberg University); Suraj Nair (Stanford University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[biagio.brattoli@gmail.com; surajn@stanford.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ishan Misra (Facebook AI Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[imisra@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[54]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 1:17:26 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 4:59:10 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-training algorithms, which train a model to fit pseudolabels predicted by another previously-learned model, have been very successful for learning with unlabeled data using neural networks. However, the current theoretical understanding of self-training only applies to linear models. This work provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. At the core of our analysis is a simple but realistic ``expansion'' assumption, which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. We also assume that neighborhoods of examples in different classes have minimal overlap. We prove that under these assumptions, the minimizers of population objectives based on self-training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, we immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness. Our results help explain the empirical successes of recently proposed self-training algorithms which use input consistency regularization.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Colin Wei]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[colinwei@stanford.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Colin Wei (Stanford University)*; Kendrick Shen (Stanford University); Yining Chen (Stanford University); Tengyu Ma (Stanford)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wei, Colin*; Shen, Kendrick; Chen, Yining; Ma, Tengyu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[colinwei@stanford.edu*; kshen6@stanford.edu; cynnjjs@stanford.edu; tengyuma@stanford.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[workshop.pdf (1,739,853 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ke Sun (Peking University); Santhosh Kumar Ramakrishnan (University of Texas at Austin)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ajksunke@pku.edu.cn; srama@cs.utexas.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wenzhen Yuan ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuanwz@cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[57]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 2:41:08 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:00:08 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Contrast and Classify: Alternate Training for Robust VQA]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Visual Question Answering (VQA) models have shown impressive performance on the VQA benchmark but remain sensitive to small linguistic variations in input questions. Existing approaches address this by augmenting the dataset with question paraphrases from visual question generation models or adversarial perturbations. These approaches use the combined data to learn an answer classifier by minimizing the standard cross-entropy loss. To more effectively leverage the augmented data, we build on the recent success in contrastive learning. We propose a novel training paradigm (ConCAT) that alternately optimizes cross-entropy and contrastive losses. The contrastive loss encourages representations to be robust to linguistic variations in questions while the cross-entropy loss preserves the discriminative power of the representations for answer classification. VQA models trained with ConCAT achieve higher consensus scores on the VQA-Rephrasings dataset as well as higher VQA accuracy on the VQA 2.0 dataset.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yash Mukund Kant]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ysh.kant@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yash Mukund Kant (Georgia Institute of Technology)*; Abhinav Moudgil (Georgia Institute of Technology); Dhruv Batra (Georgia Tech & Facebook AI Research); Devi Parikh (Georgia Tech & Facebook AI Research); Harsh Agrawal (Georgia Institute of Technology)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kant, Yash Mukund*; Moudgil, Abhinav; Batra, Dhruv; Parikh, Devi; Agrawal, Harsh]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ysh.kant@gmail.com*; amoudgil3@gatech.edu; dbatra@gatech.edu; parikh@gatech.edu; harsh.agrawal@gatech.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[paper.pdf (9,940,857 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Xingyi Yang (University of California San Diego); Yue Wang (Massachusetts Institute of Technology)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[x3yang@eng.ucsd.edu; yuewang@csail.mit.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[59]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 3:07:50 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:03:56 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Predicting What You Already Know Helps: Provable Self-Supervised Learning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Self-supervised representation learning solves auxiliary prediction tasks (known as pretext tasks), that do not require labeled data, to learn semantic representations. These pretext tasks are created solely using the input features, such as predicting a missing image patch, recovering the color channels of an image from context, or predicting missing words in text, yet predicting this \textit{known} information helps in learning representations effective for downstream prediction tasks. This paper posits a mechanism based on approximate conditional independence to formalize how solving certain pretext tasks can learn representations that provably decrease the sample complexity of downstream supervised tasks. Formally, we quantify how the approximate independence between the components of the pretext task (conditional on the label and latent variables) allows us to learn representations that can solve the downstream task with drastically reduced sample complexity by just training a linear layer on top of the learned representation. ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Qi Lei]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[qilei@princeton.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jason Lee (Princeton); Qi Lei (Princeton University)*; Nikunj Saunshi (Princeton University); Jiacheng Zhuo (University of Texas at Austin)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Lee, Jason; Lei, Qi*; Saunshi, Nikunj; Zhuo, Jiacheng]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jasonlee@princeton.edu; qilei@princeton.edu*; nsaunshi@cs.princeton.edu; jzhuo@utexas.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSL_supplementary.pdf (1,418,255 bytes); SSL_workshop.pdf (287,392 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Haoyu Ma (University of California, Irvine); Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[haoyum3@uci.edu; p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[61]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 5:15:30 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 10:46:49 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Environment Predictive Coding for Embodied Agents]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents. In contrast to prior work on self-supervised learning for images, we aim to jointly encode a series of images gathered by an agent as it moves about in 3D environments. We learn these representations via a zone prediction task, where we intelligently mask out portions of an agent's trajectory and predict them from the unmasked portions, conditioned on the agent's camera poses. By learning such representations on a collection of videos, we demonstrate successful transfer to multiple downstream navigation-oriented tasks. Our experiments on the photorealistic 3D environments of Gibson and Matterport3D show that our method outperforms the state-of-the-art on challenging tasks with only a limited budget of experience.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Santhosh Kumar Ramakrishnan]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[sramakrishnan@utexas.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Santhosh Kumar Ramakrishnan (University of Texas at Austin)*; Tushar Nagarajan (UT Austin); Ziad Al-Halah (University of Texas at Austin); Kristen Grauman (Facebook AI Research & UT Austin)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ramakrishnan, Santhosh Kumar*; Nagarajan, Tushar; Al-Halah, Ziad; Grauman, Kristen]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[sramakrishnan@utexas.edu*; tushar@cs.utexas.edu; ziad@cs.utexas.edu; grauman@cs.utexas.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[3]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[epc_ssl_workshop_compressed.pdf (4,368,753 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jiangliu WANG (CUHK); Junhwa Hur (TU Darmstadt)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jiangliuwang@link.cuhk.edu.hk; junhwa.hur@visinf.tu-darmstadt.de]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shanghang Zhang (UC Berkeley)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[shz@cs.berkeley.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[63]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 6:07:50 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:04:44 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Supervision Accelerates Pre-training in Contrastive Semi-Supervised Learning of Visual Representations]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We investigate a strategy for improving the efficiency of contrastive learning of visual representations by leveraging a small amount of supervised information during pre-training. We propose a semi-supervised loss, SuNCEt, based on noise-contrastive estimation and neighbourhood component analysis, that aims to distinguish examples of different classes in addition to the self-supervised instance-wise pretext tasks. On ImageNet, we find that SuNCEt can be used to match the semi-supervised learning accuracy of previous contrastive approaches while using less than half the amount of pre-training and compute. Our main insight is that leveraging even a small amount of labeled data during pre-training, and not only during fine-tuning, provides an important signal that can significantly accelerate contrastive learning of visual representations.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Mahmoud Assran]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[mido.sherif.assran@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Mahmoud Assran (McGill University; Mila; Facebook AI Research)*; Nicolas Ballas (Facebook FAIR); Lluis Castrejon (Mila, Université de Montréal, Facebook AI Research); Mike Rabbat (Facebook FAIR)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Assran, Mahmoud*; Ballas, Nicolas; Castrejon, Lluis; Rabbat, Mike]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[mido.sherif.assran@gmail.com*; ballasn@fb.com; lluis.enric.castrejon.subira@umontreal.ca; mikerabbat@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[suncet.pdf (4,694,250 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ruohan Gao (University of Texas at Austin); Zhiqiu Lin ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[rhgao@cs.utexas.edu; zhiqiul@andrew.cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wenzhen Yuan ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuanwz@cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[64]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 6:08:41 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:04:59 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Understanding self-supervised learning using controlled datasets with known structure]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Modern self-supervised learning techniques such as SimCLR can learn surprisingly good representations from unlabeled images. However, little is known regarding how the learned representations are organized. Here, we propose to investigate the representations learned by contrastive self-supervised models on synthetically generated datasets where all structure is known. We investigate how SimCLR represents data with discrete, hierarchically structured classes, showing that the distances between examples in the representation space reflect the hierarchy. We also probe the representation of continuous appearance parameters. We demonstrate that the representations learned in a simplified setting capture several of the properties of representations learned from ImageNet, and study systematically the interactions between augmentations and continuous features. Our experiments show that contrastive learning methods exhibit non-trivial behavior even on simple datasets where the origins of this behavior may be more tractably investigated.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Katherine L. Hermann]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[hermannk@stanford.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Katherine L. Hermann (Stanford University)*; Ting Chen (Google); Mohammad Norouzi (Google Research, Brain Team); Simon Kornblith (Google Brain)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Hermann, Katherine L.*; Chen, Ting; Norouzi, Mohammad; Kornblith, Simon]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[hermannk@stanford.edu*; iamtingchen@google.com; mnorouzi@google.com; skornblith@google.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Understanding_selfsupervised_learning.pdf (2,275,698 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Peter West (University of Washington); Yuning You (Texas A&M University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[pawest@cs.washington.edu; yuning.you@tamu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ishan Misra (Facebook AI Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[imisra@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[65]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 6:21:10 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:05:13 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Greedy Hierarchical Variational Autoencoders for Scalable Visual Dynamics Models]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accurately predicting and reasoning about the action-conditioned visual future remains an important challenge for scalable offline robot learning. Although state-of-the-art action-conditioned video prediction algorithms have produced promising results for small robotic manipulation datasets, they have yet to scale up to increasingly larger datasets in order to generalize across domains, tasks, and robots. This paper introduces Greedy Hierarchical VAEs (GHVAEs), which performs scalable action-conditioned video prediction by merging two ingredients essential to the observed performance gains - greedy machine learning and hierarchical VAEs. The fusion of these two algorithmic concepts produces a scalable machine learning algorithm that can learn multi-level visual representations of the physical world from ever-larger offline video datasets. To begin, we observe that the practical ability to train bigger video prediction models for larger datasets is often bottlenecked by the memory constraints of the GPUs or TPUs. To this end, we introduce a greedy machine learning algorithm that achieves higher video prediction accuracy when compared to the same model that is optimized end-to-end. Secondly, we posit that while hierarchical VAEs can in theory extract multi-level visual representations, training them end-to-end often leads to optimization difficulties, and that greedy machine learning can be a natural solution to such an optimization dilemma. Finally, we experimentally evaluate GHVAEs against state-of-the-art video prediction methods on large-scale action-conditioned video prediction datasets and report results that reveal insights and intuitions about this algorithm.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Bohan Wu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[bohan.wu@cs.stanford.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Bohan Wu (Stanford University)*; Suraj Nair (Stanford University); Roberto Martín-Martín (Stanford University); Chelsea Finn (Stanford); Li Fei-Fei (Stanford University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wu, Bohan*; Nair, Suraj; Martín-Martín, Roberto; Finn, Chelsea; Fei-Fei, Li]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[bohan.wu@cs.stanford.edu*; surajn@stanford.edu; roberto.martinmartin@stanford.edu; cbfinn@cs.stanford.edu; feifeili@cs.stanford.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Greedy Hierarchical Variational Autoencoders for Scalable Visual Dynamics Models (Supplementary Materials).pdf (255,098 bytes); Greedy Hierarchical Variational Autoencoders for Scalable Visual Dynamics Models.pdf (856,682 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Shikun Liu (Imperial College London); Zihang Lai (University of Oxford)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[sk.lorenmt@gmail.com; zihang.lai@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[66]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 6:36:18 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:05:37 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pre-training Text-to-Text Transformers to Write and Reason with Concepts]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pretrained language models (PTLM) have achieved impressive results in natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective. Our proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task).]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Dong-Ho Lee]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[dongho.lee@usc.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wangchunshu Zhou (Beihang University); Dong-Ho Lee (University of Southern California)*; Ravi Kiran Selvam (University of Southern California); Seyeon Lee (University of Southern California); Yuchen Lin (University of Southern California); Xiang Ren (University of Southern California)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Zhou, Wangchunshu; Lee, Dong-Ho*; Selvam, Ravi Kiran; Lee, Seyeon; Lin, Yuchen; Ren, Xiang]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[zhouwangchunshu@buaa.edu.cn; dongho.lee@usc.edu*; rselvam@usc.edu; seyeonle@usc.edu; yuchen.lin@usc.edu; xiangren@usc.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[NIPS20_Workshop__CALM.pdf (1,268,670 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Hankook Lee (KAIST); Jae Shin Yoon (U of Minnesota)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[hankook.lee@kaist.ac.kr; jsyoon@umn.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Ishan Misra (Facebook AI Research)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[imisra@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[67]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 8:39:27 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:06:00 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Contrastive Learning can Identify the Underlying Generative Factors of the Data]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Contrastive representation learning has recently seen tremendous successes in unsupervised learning, but we do not yet understand what representations are being learned and why they generalize so well to a large variety of downstream tasks. In this work, we show both theoretically and empirically that feedforward models trained on a contrastive loss implicitly invert the generative model of the data for certain data distributions (up to affine transformations). This result highlights a deep connection between contrastive learning, generative modeling, and nonlinear ICA, and provides a theoretical foundation to derive more effective contrastive learning losses and to further our understanding of the learned representations.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Roland S. Zimmermann]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[roland.zimmermann@bethgelab.org]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Roland S. Zimmermann (University of Tuebingen)*; Steffen Schneider (University of Tübingen); Yash Sharma (University of Tübingen); Matthias Bethge (University of Tübingen); Wieland Brendel (University of Tübingen)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Zimmermann, Roland S.*; Schneider, Steffen; Sharma, Yash; Bethge, Matthias; Brendel, Wieland]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[roland.zimmermann@bethgelab.org*; steffen.schneider@bethgelab.org; yash.sharma@bethgelab.org; matthias.bethge@uni-tuebingen.de; wieland.brendel@bethgelab.org]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[appendix.pdf (282,063 bytes); main.pdf (280,841 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Biagio Brattoli (Heidelberg University); Shaolei Wang (Harbin Institute of Technology)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[biagio.brattoli@gmail.com; slwang@ir.hit.edu.cn]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Wenzhen Yuan ()]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuanwz@cmu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[73]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/10/2020 11:59:42 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:07:04 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Evaluation of Out-of-Distribution Detection Performance of Self-Supervised Learning in a Controllable Environment]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[We evaluate the out-of-distribution (OOD) detection performance of self-supervised learning (SSL) techniques with a new evaluation framework. Unlike the previous evaluation methods, the proposed framework adjusts the distance of OOD samples from the in-distribution samples. We evaluate an extensive combination of OOD detection algorithms on three different implementations of the proposed framework using simulated samples and images. SSL methods consistently demonstrated the improved OOD detection performance in all evaluation settings.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jeonghoon Park]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jeonghoon_park@kaist.ac.kr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jeonghoon Park (Korea Advanced Institute of Science and Technology)*; Kyungmin Jo (Korea Advanced Institute of Science and Technology); Daehoon Gwak (Korea University); Jimin Hong (Humelo); Jaegul Choo (Korea Advanced Institute of Science and Technology); Edward Choi (KAIST)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Park, Jeonghoon*; Jo, Kyungmin; Gwak, Daehoon; Hong, Jimin; Choo, Jaegul; Choi, Edward]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jeonghoon_park@kaist.ac.kr*; bttkm@kaist.ac.kr; hune282@korea.ac.kr; jimin9401@gmail.com; jchoo@kaist.ac.kr; edwardchoi@kaist.ac.kr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[NuerIPS_SSL_OOD Final.pdf (589,747 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yuhua Chen (ETH Zurich); Yuning You (Texas A&M University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yuhua.chen@vision.ee.ethz.ch; yuning.you@tamu.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Pengtao Xie (UC San Diego)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[p1xie@eng.ucsd.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[76]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/14/2020 12:36:48 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:07:19 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Hard Negative Mixing for Contrastive Learning]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both 
crucial in learning such representations. At the same time, data mixing strategies, either at the image or the feature level, improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, \ie the effect of \emph{hard negatives}, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing memory requirements, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose {\em hard negative mixing} strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection, and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yannis Kalantidis]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ykalant@image.ntua.gr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yannis Kalantidis (NAVER Labs Europe)*; Mert Bulent SARIYILDIZ  (NAVER Labs Europe); Noe Pion (NAVER Labs Europe); Philippe Weinzaepfel (NAVER LABS Europe); Diane Larlus (Naver Labs Europe)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kalantidis, Yannis*; SARIYILDIZ , Mert Bulent; Pion, Noe; Weinzaepfel, Philippe; Larlus, Diane]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[ykalant@image.ntua.gr*; mertbulent.sariyildiz@naverlabs.com; noe.pion@gmail.com; philippe.weinzaepfel@naverlabs.com; diane.larlus@naverlabs.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Hard_Negative_Mixing___4_pages.pdf (722,168 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Kai Han (University of Oxford); Kyungjune Baek (Yonsei University)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[khan@robots.ox.ac.uk; bkjbkj12@yonsei.ac.kr]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abdel-rahman Mohamed (Facebook AI Research (FAIR))]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abdo@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[77]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/16/2020 9:43:42 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[10/30/2020 5:07:31 PM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Generalized Adversarially Learned Inference]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Allowing effective inference of latent vectors while training GANs can greatly increase their applicability in various downstream tasks. Recent approaches, such as ALI and BiGAN frameworks, develop methods of inference of latent variables in GANs by adversarially training an image generator along with an encoder to match two joint distributions of image and latent vector pairs. We generalize these approaches to incorporate multiple layers of feedback on reconstructions as well as other forms of self-supervised feedback. We achieve this by modifying the discriminator's objective to correctly identify more than two joint distributions of tuples of an arbitrary number of random variables consisting of images, latent vectors, and other variables generated through auxiliary tasks, such as reconstruction and inpainting or as outputs of suitable pre-trained models. Within our proposed framework, we introduce a novel set of techniques for providing self-supervised feedback to the model based on properties, such as patch-level correspondence and cycle consistency of reconstructions. Through comprehensive experiments, we demonstrate the efficacy, scalability, and flexibility of the proposed approach for a variety of tasks.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yatin Dandi]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yatind@iitk.ac.in]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yatin Dandi (IIT Kanpur)*; Homanga Bharadhwaj (University of Toronto, Vector Institute); Abhishek Kumar (Google Brain); Piyush Rai (IIT Kanpur)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Dandi, Yatin*; Bharadhwaj, Homanga; Kumar, Abhishek; Rai, Piyush]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[yatind@iitk.ac.in*; homanga@cs.toronto.edu; abhishk@google.com; piyush@cse.iitk.ac.in]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[SSLNeurIPS2020]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[2]]></Data>
        </Cell>
        <Cell ss:StyleID="s2">
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Disabled (0)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[neurips_SSL_GALI.pdf (1,979,267 bytes)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[0]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Jialong Tang (Institute of Software Chinese Academy of Sciences); Kai Han (University of Oxford)]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[jialong2019@iscas.ac.cn; khan@robots.ox.ac.uk]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Abdel-rahman Mohamed (Facebook AI Research (FAIR))]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[abdo@fb.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
    </Table>
  </Worksheet>
</Workbook>