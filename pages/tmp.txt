<li><b><u>Self-Supervised Learning for Large-Scale Unsupervised Image Clustering</u></b> - <i>Evgenii Zheltonozhskii (Technion), Chaim Baskin (Technion), Alex Bronstein (Technion), Avi Mendelson (Technion)</i></li><li><b><u>Learning Self-Expression Metrics for Scalable and Inductive Subspace Clustering</u></b> - <i>Julian Busch (LMU Munich), Evgeniy Faerman (Ludwig Maximilian University of Munich), Matthias Schubert (Ludwig-Maximilians-Universität München), Thomas Seidl (LMU Munich)</i></li><li><b><u>Self-alignment Pre-training for Biomedical Entity Representations</u></b> - <i>Fangyu Liu (University of Cambridge), Ehsan Shareghi (UCL, University of Cambridge), Zaiqiao Meng (University of Glasgow), Marco Marco Basaldella (University of Cambridge), Nigel Collier (University of Cambridge)</i></li><li><b><u>Prototypical Contrastive Learning of Unsupervised Representations</u></b> - <i>Junnan Li (Salesforce), Pan Zhou (Salesforce), Caiming Xiong (Salesforce Research), Steven Hoi (Salesforce)</i></li><li><b><u>i-Mix: A Strategy for Regularizing Contrastive Representation Learning</u></b> - <i>Kibok Lee (University of Michigan), Yian Zhu (University of Michigan	), Kihyuk Sohn (Google), Chun-Liang Li (Google), Jinwoo Shin (KAIST), Honglak Lee (University of Michingan / Google Research)</i></li><li><b><u>MixCo: Mix-up Contrastive Learning for Visual Representation</u></b> - <i>Gihun Lee (KAIST), Sungnyun Kim (KAIST), Sangmin Bae (KAIST), Se-Young Yun (KAIST)</i></li><li><b><u>Self-supervised Representation Learning with Relative Predictive Coding</u></b> - <i>Yao-Hung Tsai (Carnegie Mellon University), Martin Ma (Carnegie Mellon University), Muqiao Yang (Carnegie Mellon University), Han Zhao (Carnegie Mellon University), Ruslan Salakhutdinov (Carnegie Mellon University), Louis-Philippe Morency (Carnegie Mellon University)</i></li><li><b><u>A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks</u></b> - <i>Nikunj Saunshi (Princeton University), Sadhika Malladi (Princeton), Sanjeev Arora (Princeton University)</i></li><li><b><u>SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning</u></b> - <i>Byoungjip Kim (Samsung SDS), Jinho Choo (Samsung SDS), Yeong-Dae Kwon (Samsung SDS), Seongho Joe (samsung SDS), Seungjai Min (Samsung SDS), Youngjune L Gwon (Samsung SDS)</i></li><li><b><u>Spatiotemporal Contrastive Video Representation Learning</u></b> - <i>Rui Qian (Cornell University), Tianjian Meng (Google Brain), Boqing Gong (Google), Ming-Hsuan Yang (Google Research), Huisheng Wang (Google), Serge Belongie (Cornell University), Yin Cui (Google)</i></li><li><b><u>Make Lead Bias in Your Favor: Zero-shot Abstractive News Summarization</u></b> - <i>Chenguang Zhu (Microsoft), Ziyi Yang (Stanford University), Robert Gmyr (Microsoft), Michael Zeng (Microsoft), Xuedong Huang (Microsoft)</i></li><li><b><u>Semantic Augmentation with Self-Supervised Content Mixing for Semi-Supervised Learning</u></b> - <i>Remy Sun (Laboratoire d'informatique de Paris 6), Clément Masson (Thales Land and Air Systems), Gilles Henaff (Thales Optronique S.A.S.), Nicolas Thome (Cnam, CEDRIC), Matthieu Cord (Sorbonne University)</i></li><li><b><u>Self-Supervised Object-Wise 3D Decomposition of Images using Shape Priors</u></b> - <i>Cathrin Elich (Max Planck Institute for Intelligent Systems), Martin R. Oswald (ETH Zurich), Marc Pollefeys (ETH Zurich / Microsoft), Joerg Stueckler (Max-Planck-Institute for Intelligent Systems)</i></li><li><b><u>Contrastive Learning with Hard Negative Samples</u></b> - <i>Joshua D Robinson (MIT), Ching-Yao  Chuang (MIT), Suvrit Sra (Massachusetts Institute of Technology, USA), Stefanie  Jegelka (MIT)</i></li><li><b><u>BYOL works even without batch statistics</u></b> - <i>Pierre H. Richemond (DeepMind), Jean-Bastien Grill (DeepMind), Florent Altché (DeepMind), Corentin Tallec (Deepmind), Florian Strub (DeepMind), Bilal Piot (DeepMind), Michal Valko (DeepMind)</i></li><li><b><u>Self-supervised Learning from a Multi-view Perspective</u></b> - <i>Yao-Hung Tsai (Carnegie Mellon University), Yue Wu (Carnegie Mellon University), Ruslan Salakhutdinov (Carnegie Mellon University), Louis-Philippe Morency (Carnegie Mellon University)</i></li><li><b><u>Refining Pre-trained NLP Models Through Shuffled-token Detection</u></b> - <i>Subhadarshi Panda (Graduate Center City University of New York), Anjali Agrawal (New York University), Jeewon Ha (New York University), Benjamin Bloch (New York University), Samuel R. Bowman (New York University)</i></li><li><b><u>Understanding Self-supervised Learning with Dual Deep Networks</u></b> - <i>Yuandong Tian (Facebook), Lantao Yu (Stanford University), Xinlei Chen (Facebook AI Research), Surya Ganguli ()</i></li><li><b><u>Visual Question Answering with Annotation-Efficient Zero Shot Learning under Linguistic Domain Shift</u></b> - <i>Pratyay Banerjee (Arizona State University), Tejas Gokhale (Arizona State University), Yezhou Yang (Arizona State University), Chitta Baral (Arizona State University)</i></li><li><b><u>Data Transformation Insights in Self-Supervision with Clustering Tasks</u></b> - <i>Abhimanu Kumar (Microsoft), Aniket Anand Deshmukh (Microsoft), Urun Dogan (Microsoft), Denis Charles (Microsoft), Eren Manavoglu ()</i></li><li><b><u>Functional Regularization for Representation Learning: A Unified Theoretical Perspective</u></b> - <i>Siddhant Garg (Amazon Alexa AI Search), Yingyu Liang (University of Wisconsin Madison)</i></li><li><b><u>Cross-Domain Sentiment Classification With In-domain Contrastive Learning</u></b> - <i>Tian Li (Peking University), Xiang Chen (Peking University), Zhen Dong (UC Berkeley), Shanghang Zhang (UC Berkeley), Kurt Keutzer (UC Berkeley)</i></li><li><b><u>Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency</u></b> - <i>Qiang Zhang (Shanghai Jiao Tong University), Tete Xiao (UC Berkeley), Alexei A Efros (UC Berkeley), Lerrel Pinto (NYU/Berkeley), Xiaolong Wang (UCSD)</i></li><li><b><u>Pretraining Neural Architecture Search Controllers with Locality-based Self-Supervised Learning</u></b> - <i>Kwanghee Choi (Sogang University), Minyoung Choe (Sogang university), Hyelee Lee (NAVER)</i></li><li><b><u>Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision</u></b> - <i>Yujie Zhong (University of Oxford), Yishu Miao (Imperial College London), Linhai Xie (University of Oxford), Sen Wang (Heriot-Watt University)</i></li><li><b><u>Representation Learning via Invariant Causal Mechanisms</u></b> - <i>Jovana Mitrovic (DeepMind), Brian McWilliams (DeepMind), Jacob Walker (DeepMind), Lars Buesing (DeepMind), Charles  Blundell (DeepMind)</i></li><li><b><u>SSD: A Unified Framework for Self-Supervised Outlier Detection</u></b> - <i>Vikash Sehwag (Princeton University), Mung Chiang (Princeton University), Prateek Mittal (Princeton University)</i></li><li><b><u>Is It a Plausible Colour? UCapsNet for Colourisation</u></b> - <i>Rita Pucci (University of Udine), CHRISTIAN MICHELONI (University of Udine, Italy), Gian Luca Foresti (University of Udine, Italy), Niki Martinel (University of Udine)</i></li><li><b><u>Self-guided Approximate Linear Programs</u></b> - <i>Parshan Pakiman (University of Illinois at Chicago), Selvaprabu Nadarajah (	University of Illinois at Chicago), Negar Soheili Azad (University of Illinois at Chicago), Qihang Lin (University of Iowa)</i></li><li><b><u>SemanticCMC - improved semantic self-supervised learning with naturalistic temporal co-occurrences</u></b> - <i>Cliona O'Doherty (Trinity College Dublin), Rhodri Cusack (Trinity College Dublin)</i></li><li><b><u>Social NCE: Contrastive Learning for Socially-aware Trajectory Forecasting and Motion Planning</u></b> - <i>Yuejiang Liu (EPFL), Qi Yan (EPFL), Alexandre Alahi (EPFL)</i></li><li><b><u>Self-Supervised Ranking for Representation Learning</u></b> - <i>Ali Varamesh (KU Leuven), Ali Diba (KU Leuven), Tinne Tuytelaars (KU Leuven), Luc Van Gool (ETH Zurich)</i></li><li><b><u>Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data</u></b> - <i>Colin Wei (Stanford University), Kendrick Shen (Stanford University), Yining Chen (Stanford University), Tengyu Ma (Stanford)</i></li><li><b><u>Contrast and Classify: Alternate Training for Robust VQA</u></b> - <i>Yash Mukund Kant (Georgia Institute of Technology), Abhinav Moudgil (Georgia Institute of Technology), Dhruv Batra (Georgia Tech & Facebook AI Research), Devi Parikh (Georgia Tech & Facebook AI Research), Harsh Agrawal (Georgia Institute of Technology)</i></li><li><b><u>Predicting What You Already Know Helps: Provable Self-Supervised Learning</u></b> - <i>Jason Lee (Princeton), Qi Lei (Princeton University), Nikunj Saunshi (Princeton University), Jiacheng Zhuo (University of Texas at Austin)</i></li><li><b><u>Environment Predictive Coding for Embodied Agents</u></b> - <i>Santhosh Kumar Ramakrishnan (University of Texas at Austin), Tushar Nagarajan (UT Austin), Ziad Al-Halah (University of Texas at Austin), Kristen Grauman (Facebook AI Research & UT Austin)</i></li><li><b><u>Supervision Accelerates Pre-training in Contrastive Semi-Supervised Learning of Visual Representations</u></b> - <i>Mahmoud Assran (McGill University, Mila, Facebook AI Research), Nicolas Ballas (Facebook FAIR), Lluis Castrejon (Mila, Université de Montréal, Facebook AI Research), Mike Rabbat (Facebook FAIR)</i></li><li><b><u>Understanding self-supervised learning using controlled datasets with known structure</u></b> - <i>Katherine L. Hermann (Stanford University), Ting Chen (Google), Mohammad Norouzi (Google Research, Brain Team), Simon Kornblith (Google Brain)</i></li><li><b><u>Greedy Hierarchical Variational Autoencoders for Scalable Visual Dynamics Models</u></b> - <i>Bohan Wu (Stanford University), Suraj Nair (Stanford University), Roberto Martín-Martín (Stanford University), Chelsea Finn (Stanford), Li Fei-Fei (Stanford University)</i></li><li><b><u>Pre-training Text-to-Text Transformers to Write and Reason with Concepts</u></b> - <i>Wangchunshu Zhou (Beihang University), Dong-Ho Lee (University of Southern California), Ravi Kiran Selvam (University of Southern California), Seyeon Lee (University of Southern California), Yuchen Lin (University of Southern California), Xiang Ren (University of Southern California)</i></li><li><b><u>Contrastive Learning can Identify the Underlying Generative Factors of the Data</u></b> - <i>Roland S. Zimmermann (University of Tuebingen), Steffen Schneider (University of Tübingen), Yash Sharma (University of Tübingen), Matthias Bethge (University of Tübingen), Wieland Brendel (University of Tübingen)</i></li><li><b><u>Evaluation of Out-of-Distribution Detection Performance of Self-Supervised Learning in a Controllable Environment</u></b> - <i>Jeonghoon Park (Korea Advanced Institute of Science and Technology), Kyungmin Jo (Korea Advanced Institute of Science and Technology), Daehoon Gwak (Korea University), Jimin Hong (Humelo), Jaegul Choo (Korea Advanced Institute of Science and Technology), Edward Choi (KAIST)</i></li><li><b><u>Hard Negative Mixing for Contrastive Learning</u></b> - <i>Yannis Kalantidis (NAVER Labs Europe), Mert Bulent SARIYILDIZ  (NAVER Labs Europe), Noe Pion (NAVER Labs Europe), Philippe Weinzaepfel (NAVER LABS Europe), Diane Larlus (Naver Labs Europe)</i></li><li><b><u>Generalized Adversarially Learned Inference</u></b> - <i>Yatin Dandi (IIT Kanpur), Homanga Bharadhwaj (University of Toronto, Vector Institute), Abhishek Kumar (Google Brain), Piyush Rai (IIT Kanpur)</i></li>